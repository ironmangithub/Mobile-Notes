## NCHW
batch N, channels C, depth D, height H, width W

## Opset NUmber
It is a kind of version number named by ONNX. Every library is versioned. ONNX does also have a version called opset number. Operator ArgMin was added in opset 1 and changed in opset 11, 12, 13.

--------------------------------------------------------------------
## Hugging Face
### Definition
The **AI community** building the future. **Build, train and deploy state of the art models** powered by the reference open source in machine learning.
### Link
* https://huggingface.co/
* https://github.com/huggingface

------------------------------------------------------------------
# MLCommons Inference Benchmarks

## resnet50
### Basics:
* 

## bert
### Basics
* for natural language processing.
* BERT-Large performing SQuAD v1.1 **question answering task**.

 
## 3d-unet-kits19
### Basics:
* for medical image 3D segmentation: kidney tumor segmentation task.


## dlrm
### Basics: 
* for recommencation task
-----------------------------------------------------------------
reference: https://pytorch.org/tutorials/beginner/saving_loading_models.html
# PyTorch Saving and Loading Models
## General Concepts
### torch.save
saves a **serialized object** to disk.
### torch.load
uses pickle's unpickling facilities to **deserialize pickled object** files to memory. 
### torch.nn.Module.load_state_dict
1. loads a model's **parameter dictionary** using a **deserialized state_dict**.
2. A state_dict is simply a **Python dictionary object** that **maps each layer to its parameter tensor**.
Optimizer objects (torch.optim) also have a state_dict, which contains information about the optimizer’s state, as well as the hyperparameters used.
## Two Approaches
### state_dict
1. save
torch.save(model.state_dict(), PATH)
2. load
model = TheModelClass(*args, **kwargs)
model.load_state_dict(torch.load(PATH))
model.eval()
### entire model
1. save:
torch.save(model, PATH)
2. load
model = torch.load(PATH)
model.eval()

# TorchScript Format
## TorchScript Definition
an **intermediate representation** of a **PyTorch model** that can be run in **Python** as well as in a high performance environment **like C++**.
## Export to TorchScript
### Export
model_scripted = torch.jit.script(model) # Export to TorchScript
model_scripted.save('model_scripted.pt') # Save
### Load
model = torch.jit.load('model_scripted.pt')
model.eval()
## torch.jit.script
* Usage: Scripting a function or nn.Module will inspect the source code, **compile it as TorchScript code** using the TorchScript compiler, and return a **ScriptModule** or **ScriptFunction**. 
* Codes: 
## torch.jit.save
* Usage: save ScriptModule. 
* Codes:
m = torch.jit.script(MyModule())
torch.jit.save(m, 'scriptmodule.pt')
## torch.jit.load
* Usage: Load a **ScriptModule** or **ScriptFunction** previously saved with **torch.jit.save**. 
* Returns: A ScriptModule object.
* Codes:
torch.jit.load('scriptmodule.pt')

# model.eval（）
1. you must call model.eval() to **set dropout and batch normalization layers** to evaluation mode **before running inference**. 
2. Failing to do this will yield **inconsistent inference results**.

----------------------------------------------------------------
# MLPerf Inference Rules
Reference: https://github.com/mlcommons/inference_policies/blob/master/inference_rules.adoc
## Overview
### Definitions
